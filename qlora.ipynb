{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Finetuning on IMDb Sentiment Dataset using GPT-Neo 1.3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:24:16.969628Z",
     "iopub.status.busy": "2025-04-10T11:24:16.969327Z",
     "iopub.status.idle": "2025-04-10T11:24:47.660558Z",
     "shell.execute_reply": "2025-04-10T11:24:47.659794Z",
     "shell.execute_reply.started": "2025-04-10T11:24:16.969605Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-gchuq0ej\n",
      "\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-4n1q4e9l\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install required libraries \n",
    "import subprocess\n",
    "subprocess.run([\"pip\", \"install\", \"-U\", \"bitsandbytes>=0.43.0\"], stdout=subprocess.DEVNULL)\n",
    "subprocess.run([\"pip\", \"install\", \"-U\", \"git+https://github.com/huggingface/transformers.git\"], stdout=subprocess.DEVNULL)\n",
    "subprocess.run([\"pip\", \"install\", \"-U\", \"git+https://github.com/huggingface/accelerate.git\"], stdout=subprocess.DEVNULL)\n",
    "subprocess.run([\"pip\", \"install\", \"-U\", \"peft\"], stdout=subprocess.DEVNULL)\n",
    "\n",
    "print(\"All required packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:48:00.533393Z",
     "iopub.status.busy": "2025-04-10T09:48:00.532930Z",
     "iopub.status.idle": "2025-04-10T09:48:05.987338Z",
     "shell.execute_reply": "2025-04-10T09:48:05.986761Z",
     "shell.execute_reply.started": "2025-04-10T09:48:00.533369Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 2. Load model and tokenizer (4-bit quantized using BitsAndBytesConfig)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, logging\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Disable unnecessary generation logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print('Model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T09:48:07.684454Z",
     "iopub.status.busy": "2025-04-10T09:48:07.684175Z",
     "iopub.status.idle": "2025-04-10T09:48:07.771976Z",
     "shell.execute_reply": "2025-04-10T09:48:07.771250Z",
     "shell.execute_reply.started": "2025-04-10T09:48:07.684431Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,317,148,672 || trainable%: 0.1194\n"
     ]
    }
   ],
   "source": [
    "# 3. Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"attention.q_proj\", \"attention.v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:42:59.901700Z",
     "iopub.status.busy": "2025-04-10T11:42:59.901073Z",
     "iopub.status.idle": "2025-04-10T11:43:02.926738Z",
     "shell.execute_reply": "2025-04-10T11:43:02.926016Z",
     "shell.execute_reply.started": "2025-04-10T11:42:59.901678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Tweets and their Sentiments:\n",
      "\n",
      "Tweet: [Update] Oct 30th: World Date with SHINee  Key and Jonghyun mentioned in one of cast member's tweet...\n",
      "Sentiment: Neutral\n",
      "\n",
      "Tweet: Happy Friday!!!   Watching this gives me hope in our youth. Out of all the rambling  Kanye West did this past...\n",
      "Sentiment: Positive\n",
      "\n",
      "Tweet: Murray's in major crisis here - 5 - 1 down in the 2nd set after losing the 1st! scenes!!\n",
      "Sentiment: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Load TweetEval Sentiment Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the sentiment split of TweetEval\n",
    "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
    "\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "def to_tweet_sentiment(example):\n",
    "    return {\n",
    "        \"tweet\": example[\"text\"],\n",
    "        \"sentiment\": label_map[example[\"label\"]]\n",
    "    }\n",
    "\n",
    "# Step 1: 80-20 split → train + test\n",
    "split_1 = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_val_raw = split_1[\"train\"]\n",
    "raw_test_data = split_1[\"test\"]\n",
    "\n",
    "# Step 2: 80-20 split → train + val (on the 80% training set)\n",
    "split_2 = train_val_raw.train_test_split(test_size=0.2, seed=42)\n",
    "raw_train_data = split_2[\"train\"]\n",
    "raw_val_data = split_2[\"test\"]\n",
    "\n",
    "# Apply mapping to convert to {\"tweet\", \"sentiment\"} format\n",
    "raw_train_data = raw_train_data.map(to_tweet_sentiment)\n",
    "raw_val_data = raw_val_data.map(to_tweet_sentiment)\n",
    "raw_test_data = raw_test_data.map(to_tweet_sentiment)\n",
    "\n",
    "# Visualize some examples\n",
    "print(\"\\nSample Tweets and their Sentiments:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Tweet: {raw_val_data[i]['tweet'][:300]}\")\n",
    "    print(f\"Sentiment: {raw_val_data[i]['sentiment']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:43:09.526337Z",
     "iopub.status.busy": "2025-04-10T11:43:09.525767Z",
     "iopub.status.idle": "2025-04-10T11:43:15.700393Z",
     "shell.execute_reply": "2025-04-10T11:43:15.698769Z",
     "shell.execute_reply.started": "2025-04-10T11:43:09.526313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2706fc0f7d4eb8b929d08623aaa052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f81d3c5a034085b442585b9629a53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7329c38bec4c46b7c3abdc4576e799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Tokenize \n",
    "def tokenize(example):\n",
    "    prompt = f\"Tweet: {example['tweet']}\\nSentiment: {example['sentiment']}\"\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # In next-token prediction, labels = input_ids\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    return encoded\n",
    "\n",
    "# Tokenize\n",
    "train_data = raw_train_data.map(tokenize, remove_columns=raw_train_data.column_names)\n",
    "val_data = raw_val_data.map(tokenize, remove_columns=raw_val_data.column_names)\n",
    "test_data = raw_test_data.map(tokenize, remove_columns=raw_test_data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:43:18.977696Z",
     "iopub.status.busy": "2025-04-10T11:43:18.977372Z",
     "iopub.status.idle": "2025-04-10T11:43:18.984472Z",
     "shell.execute_reply": "2025-04-10T11:43:18.983791Z",
     "shell.execute_reply.started": "2025-04-10T11:43:18.977675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pos', 'Neg', 'Ne']\n"
     ]
    }
   ],
   "source": [
    "# 6. Restrict model output to only the sentiment labels\n",
    "from transformers import LogitsProcessor\n",
    "\n",
    "class RestrictVocabLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, allowed_token_ids):\n",
    "        self.allowed_token_ids = allowed_token_ids\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        mask[:, self.allowed_token_ids] = scores[:, self.allowed_token_ids]\n",
    "        return mask\n",
    "\n",
    "sentiment_words = [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "allowed_ids = [tokenizer(word, add_special_tokens=False)[\"input_ids\"][0] for word in sentiment_words]\n",
    "logits_processor = [RestrictVocabLogitsProcessor(allowed_ids)]\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(allowed_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:43:25.399044Z",
     "iopub.status.busy": "2025-04-10T11:43:25.398547Z",
     "iopub.status.idle": "2025-04-10T11:43:25.824232Z",
     "shell.execute_reply": "2025-04-10T11:43:25.823423Z",
     "shell.execute_reply.started": "2025-04-10T11:43:25.399009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Just finished the new Spider-Man movie — absolutely loved it!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n",
      "Tweet: The product exceeded my expectations. Would definitely buy again.\n",
      "Predicted Sentiment: Positive\n",
      "Tweet: The event starts at 8 PM and runs until midnight.\n",
      "Predicted Sentiment: Negative\n",
      "Tweet: I received the package yesterday. It was exactly as described.\n",
      "Predicted Sentiment: Positive\n",
      "Tweet: This was one of the most disappointing experiences I've had.\n",
      "Predicted Sentiment: Negative\n",
      "Tweet: Terrible UI, constant crashes, and slow performance. Do better.\n",
      "Predicted Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "# 7. Sanity Check\n",
    "def predict_sentiment(review):\n",
    "    prompt = f\"Review: {review}\\nSentiment:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False,\n",
    "        logits_processor=logits_processor\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prediction = response.replace(prompt, \"\").strip().split()[0]\n",
    "    if prediction.lower().startswith(\"pos\"):\n",
    "        return \"Positive\"\n",
    "    elif prediction.lower().startswith(\"neg\"):\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "prompts = [\n",
    "    \"Tweet: Just finished the new Spider-Man movie — absolutely loved it! Sentiment:\",\n",
    "    \"Tweet: The product exceeded my expectations. Would definitely buy again. Sentiment:\",\n",
    "    \"Tweet: The event starts at 8 PM and runs until midnight. Sentiment:\",\n",
    "    \"Tweet: I received the package yesterday. It was exactly as described. Sentiment:\",\n",
    "    \"Tweet: This was one of the most disappointing experiences I've had. Sentiment:\",\n",
    "    \"Tweet: Terrible UI, constant crashes, and slow performance. Do better. Sentiment:\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(prompt[:-11])\n",
    "    print(\"Predicted Sentiment:\", predict_sentiment(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoForCausalLM(\n",
       "      (transformer): GPTNeoModel(\n",
       "        (wte): Embedding(50257, 2048)\n",
       "        (wpe): Embedding(2048, 2048)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPTNeoBlock(\n",
       "            (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTNeoAttention(\n",
       "              (attention): GPTNeoSelfAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTNeoMLP(\n",
       "              (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "              (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load LoRA configuration from checkpoint\n",
    "checkpoint_dir = \"/teamspace/studios/this_studio/checkpoints_tweeteval_lora/checkpoint-200\"  # or final model path\n",
    "peft_config = PeftConfig.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# Load base model and apply LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, checkpoint_dir)\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:43:35.152878Z",
     "iopub.status.busy": "2025-04-10T11:43:35.152168Z",
     "iopub.status.idle": "2025-04-10T11:45:58.441329Z",
     "shell.execute_reply": "2025-04-10T11:45:58.440654Z",
     "shell.execute_reply.started": "2025-04-10T11:43:35.152852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating before fine-tuning: 100%|██████████| 9123/9123 [06:53<00:00, 22.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before fine-tuning: 49.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Pretraining Evaluation: Accuracy before fine-tuning\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct = 0\n",
    "for example in tqdm(raw_test_data, desc=\"Evaluating before fine-tuning\"):\n",
    "    prediction = predict_sentiment(example[\"tweet\"])\n",
    "    if prediction.lower() == example[\"sentiment\"].lower():\n",
    "        correct += 1\n",
    "        \n",
    "accuracy = correct / len(raw_test_data)\n",
    "print(f\"Accuracy before fine-tuning: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T11:46:04.391819Z",
     "iopub.status.busy": "2025-04-10T11:46:04.391515Z",
     "iopub.status.idle": "2025-04-10T11:46:04.738450Z",
     "shell.execute_reply": "2025-04-10T11:46:04.737616Z",
     "shell.execute_reply.started": "2025-04-10T11:46:04.391796Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model saved to: ./gptneo_pre_finetuned_lora\n"
     ]
    }
   ],
   "source": [
    "# 9. Save pretrained model\n",
    "pretrained_save_path = \"./gptneo_pre_finetuned_lora\"\n",
    "\n",
    "model.save_pretrained(pretrained_save_path)\n",
    "tokenizer.save_pretrained(pretrained_save_path)\n",
    "print(f\"Pretrained model saved to: {pretrained_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T12:00:38.583700Z",
     "iopub.status.busy": "2025-04-10T12:00:38.583184Z",
     "iopub.status.idle": "2025-04-10T12:00:39.454746Z",
     "shell.execute_reply": "2025-04-10T12:00:39.454159Z",
     "shell.execute_reply.started": "2025-04-10T12:00:38.583678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Delete any existing model/data variables if needed\n",
    "#del trainer\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T12:00:47.118222Z",
     "iopub.status.busy": "2025-04-10T12:00:47.117949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17276' max='24330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17276/24330 4:26:46 < 1:48:56, 1.08 it/s, Epoch 7.10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.293000</td>\n",
       "      <td>3.321787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.171100</td>\n",
       "      <td>3.283777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.112200</td>\n",
       "      <td>3.268982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.064800</td>\n",
       "      <td>3.264747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.023800</td>\n",
       "      <td>3.265459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.989400</td>\n",
       "      <td>3.270247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.959300</td>\n",
       "      <td>3.273029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1.0/10\n",
      "No training loss recorded\n",
      "No validation loss recorded\n",
      "\n",
      "\n",
      "Epoch 2.0/10\n",
      "Train Loss: 3.2930\n",
      "Val Loss: 3.3218\n",
      "\n",
      "\n",
      "Epoch 3.0/10\n",
      "Train Loss: 3.1711\n",
      "Val Loss: 3.2838\n",
      "\n",
      "\n",
      "Epoch 4.0/10\n",
      "Train Loss: 3.1122\n",
      "Val Loss: 3.2690\n",
      "\n",
      "\n",
      "Epoch 5.0/10\n",
      "Train Loss: 3.0648\n",
      "Val Loss: 3.2647\n",
      "\n",
      "\n",
      "Epoch 6.0/10\n",
      "Train Loss: 3.0238\n",
      "Val Loss: 3.2655\n",
      "\n",
      "\n",
      "Epoch 7.0/10\n",
      "Train Loss: 2.9894\n",
      "Val Loss: 3.2702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Training\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from transformers import TrainerCallback  # Add missing import\n",
    "\n",
    "class LossPrinterCallback(TrainerCallback):  # Now properly inherits\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Find most recent training and validation losses\n",
    "        train_loss = None\n",
    "        val_loss = None\n",
    "        \n",
    "        # Search log_history in reverse order\n",
    "        for log in reversed(state.log_history):\n",
    "            if \"loss\" in log and train_loss is None:\n",
    "                train_loss = log[\"loss\"]\n",
    "            if \"eval_loss\" in log and val_loss is None:\n",
    "                val_loss = log[\"eval_loss\"]\n",
    "            if train_loss is not None and val_loss is not None:\n",
    "                break\n",
    "                \n",
    "        print(f\"\\nEpoch {state.epoch}/{state.num_train_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\" if train_loss else \"No training loss recorded\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\\n\" if val_loss else \"No validation loss recorded\\n\")\n",
    "\n",
    "\n",
    "# Updated training arguments\n",
    "training_args = TrainingArguments(\n",
    "    logging_strategy=\"epoch\",  # Log at end of each epoch\n",
    "    output_dir=\"./checkpoints_tweeteval_lora\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",  # Disable external logging\n",
    "    disable_tqdm=False  # Keep progress bars\n",
    ")\n",
    "\n",
    "# Initialize trainer with custom callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[LossPrinterCallback]\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# After training is complete\n",
    "# The best model is already loaded because you set load_best_model_at_end=True\n",
    "best_model_path = \"./best_model_tweeteval\"\n",
    "\n",
    "# Save the best model (LoRA adapter only)\n",
    "model.save_pretrained(best_model_path)\n",
    "tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "\n",
    "# To verify which checkpoint was selected as best:\n",
    "import os\n",
    "import re\n",
    "\n",
    "checkpoints = os.listdir(\"./checkpoints_tweeteval_lora\")\n",
    "checkpoint_steps = [int(re.search(r'checkpoint-(\\d+)', cp).group(1)) \n",
    "                    for cp in checkpoints if \"checkpoint\" in cp]\n",
    "if checkpoint_steps:\n",
    "    best_step = min(checkpoint_steps)  # Since we're minimizing eval_loss\n",
    "    best_epoch = best_step // len(train_data) * training_args.per_device_train_batch_size\n",
    "    print(f\"Best model was at step {best_step} (approximately epoch {best_epoch})\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
